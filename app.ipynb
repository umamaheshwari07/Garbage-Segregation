{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0de8349d-550f-4af7-b727-a2a12d626177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import random_split\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17f3c1cf-ab6a-4480-844b-451c5243fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n"
     ]
    }
   ],
   "source": [
    "data_dir  = r'.\\archive\\TrashType_Image_Dataset';\n",
    "\n",
    "classes = os.listdir(data_dir)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "780e812e-87b4-4614-9dc7-29e60a5fde68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transformations = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
    "\n",
    "dataset = ImageFolder(data_dir, transform = transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3caef5c0-4ed9-4288-94b5-7a5177a16b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_sample(img, label):\n",
    "    print(\"Label:\", dataset.classes[label], \"(Class No: \"+ str(label) + \")\")\n",
    "    plt.imshow(img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e7b8c8ca-db09-4059-bbff-07ec509b00cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x15063007d70>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "830acef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2528"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a2a958e1-230e-440d-8326-ff4880b0da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3f8825ba-9aeb-4901-8abb-57945244aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9bc6ef8c-125e-462d-b778-03653179e473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b82a42fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch {}: train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch+1, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "class ResNet(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Use a pretrained model\n",
    "        self.network = models.resnet50(pretrained=True)\n",
    "        # Replace last layer\n",
    "        num_ftrs = self.network.fc.in_features\n",
    "        self.network.fc = nn.Linear(num_ftrs, len(dataset.classes))\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return torch.sigmoid(self.network(xb))\n",
    "\n",
    "model = torch.load(\"GarbageClassifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "855731ac-eb0c-49f2-a8f4-1982aa8d76a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (network): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_device(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a93115c0-7531-4eee-b3cb-cf24f88a4a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    # Convert to a batch of 1\n",
    "    xb = to_device(img.unsqueeze(0), device)\n",
    "    # Get predictions from model\n",
    "    yb = model(xb)\n",
    "    # Pick index with highest probability\n",
    "    prob, preds  = torch.max(yb, dim=1)\n",
    "    # Retrieve the class label\n",
    "    return dataset.classes[preds[0].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f43864f2-ce29-4bbb-8cee-397fabf34732",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d01b8000-03f4-4f63-acac-cfeebe023619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "def predict_external_image(image_name):\n",
    "    image = Image.open(Path(image_name))\n",
    "\n",
    "    example_image = transformations(image)\n",
    "    plt.imshow(example_image.permute(1, 2, 0))\n",
    "    return predict_image(example_image, loaded_model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "08dd3b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ed075471",
   "metadata": {},
   "outputs": [],
   "source": [
    "classesData = {\n",
    "    'cardboard':\"\",\n",
    "    'glass': \"Glass recycling is the processing of waste glass into usable products. Glass that is crushed and ready to be remelted is called cullet. There are two types of cullet: internal and external. Internal cullet is composed of defective products detected and rejected by a quality control process during the industrial process of glass manufacturing, transition phases of product changes (such as thickness and colour changes) and production offcuts. External cullet is waste glass that has been collected or reprocessed with the purpose of recycling. External cullet (which can be pre- or post-consumer) is classified as waste. The word cullet, when used in the context of end-of-waste, will always refer to external cullet. To be recycled, glass waste needs to be purified and cleaned of contamination. Then, depending on the end use and local processing capabilities, it might also have to be separated into different colors. Many recyclers collect different colors of glass separately since glass retains its color after recycling.\",\n",
    "    'metal':\"\",\n",
    "    'paper':\"The recycling of paper is the process by which waste paper is turned into new paper products. It has a number of important benefits: It saves waste paper from occupying homes of people and producing methane as it breaks down. Because paper fibre contains carbon (originally absorbed by the tree from which it was produced), recycling keeps the carbon locked up for longer and out of the atmosphere. Around two-thirds of all paper products in the US are now recovered and recycled, although it does not all become new paper. After repeated processing, the fibres become too short for the production of new paper - this is why virgin fibre (from sustainably farmed trees) is frequently added to the pulp recipe. Paper recycling pertains to the processes of reprocessing waste paper for reuse. Waste papers are either obtained from paper mill paper scraps, discarded paper materials, and waste paper material discarded after consumer use. Examples of the commonly known papers recycled are old newspapers and magazines.\",\n",
    "    'plastic':\"Plastic recycling is the process of recovering scrap or waste plastic and reprocessing the material into useful products. Due to purposefully misleading symbols on plastic packaging and numerous technical hurdles, less than 10% of plastic has ever been recycled. Compared with the lucrative recycling of metal, and similar to the low value of glass recycling, plastic polymers recycling is often more challenging because of low density and low value. Materials recovery facilities are responsible for sorting and processing plastics. As of 2019, due to limitations in their economic viability, these facilities have struggled to make a meaningful contribution to the plastic supply chain. The plastics industry has known since at least the 1970s that recycling of most plastics is unlikely because of these limitations. However, the industry has lobbied for the expansion of recycling while these companies have continued to increase the amount of virgin plastic being produced.\",\n",
    "    'trash':\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0cd8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "\n",
    "def create_app():\n",
    "    def load_image():\n",
    "        # Open a file dialog to select an image file\n",
    "        file_path = filedialog.askopenfilename(\n",
    "            title=\"Select an Image\",\n",
    "            filetypes=[(\"Image Files\", \"*.png;*.jpg;*.jpeg;*.bmp;*.gif\")]\n",
    "        )\n",
    "        if file_path:\n",
    "            try:\n",
    "                # Load the selected image\n",
    "                img = Image.open(file_path)\n",
    "                photo = ImageTk.PhotoImage(img)\n",
    "                \n",
    "                # Update the label with the new image\n",
    "                image_label.config(image=photo)\n",
    "                image_label.image = photo  # Keep a reference to avoid garbage collection\n",
    "                \n",
    "                # Pack the image label\n",
    "                image_label.pack(pady=(0, 5))\n",
    "                \n",
    "                imgtype = predict_external_image(file_path)\n",
    "                \n",
    "                # Update the text label\n",
    "                text_label.config(text=\"Waste Classified as \\\"\" + imgtype + \"\\\" Material\")\n",
    "                text_label.pack(pady=(0, 10))  # Add some padding around the text\n",
    "\n",
    "                # Update the data label to display the string stored in 'data'\n",
    "                # Example string in 'data'\n",
    "                data = classesData[imgtype]\n",
    "                data_label.config(text=data)\n",
    "                data_label.pack(pady=(0, 10))  # Add some padding around the data label\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    # Create the main window\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Image Display\")\n",
    "    \n",
    "    # Create a button to load the image\n",
    "    load_button = tk.Button(root, text=\"Load Image\", command=load_image)\n",
    "    load_button.pack(pady=10)  # Add some padding around the button\n",
    "    \n",
    "    # Create a label to display the image\n",
    "    image_label = tk.Label(root)\n",
    "    image_label.pack(pady=(0, 5))  # Add 5 lines of space below the image\n",
    "\n",
    "    # Create a label to display the text \"This image is:\"\n",
    "    text_label = tk.Label(root, text=\"\", font=(\"Helvetica\", 16, \"bold\"))\n",
    "    text_label.pack(pady=(0, 10))  # Add some padding around the text\n",
    "\n",
    "    # Create a label to display the string stored in 'data' with word wrapping\n",
    "    data_label = tk.Label(root, text=\"\", font=(\"Helvetica\", 14), wraplength=1000, justify=\"left\")\n",
    "    data_label.pack(pady=(0, 10))  # Add some padding around the data label\n",
    "\n",
    "    # Run the Tkinter event loop\n",
    "    root.mainloop()\n",
    "\n",
    "# Call the function to create the app\n",
    "create_app()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a78bbe86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates for class 0: [[0]\n",
      " [1]\n",
      " [2]\n",
      " [4]\n",
      " [5]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 5 into shape (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 83\u001b[0m\n\u001b[0;32m     81\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mgimage.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     82\u001b[0m segmentation_mask \u001b[38;5;241m=\u001b[39m segment_objects(image_path)\n\u001b[1;32m---> 83\u001b[0m annotated_image \u001b[38;5;241m=\u001b[39m overlay_labels(image_path, segmentation_mask)\n\u001b[0;32m     84\u001b[0m annotated_image\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[1;32mIn[47], line 66\u001b[0m, in \u001b[0;36moverlay_labels\u001b[1;34m(image_path, mask)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Ensure coords is in expected format\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m coords\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m     coords \u001b[38;5;241m=\u001b[39m coords\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Ensure 2D shape if possible\u001b[39;00m\n\u001b[0;32m     68\u001b[0m min_x, min_y \u001b[38;5;241m=\u001b[39m coords\u001b[38;5;241m.\u001b[39mmin(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     69\u001b[0m max_x, max_y \u001b[38;5;241m=\u001b[39m coords\u001b[38;5;241m.\u001b[39mmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 5 into shape (2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "def preprocess(image_path):\n",
    "    transform = T.Compose([\n",
    "        T.Resize((256, 256)),  # Resize to match your model input size\n",
    "        T.ToTensor(),          # Convert to tensor\n",
    "    ])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "def segment_objects(image_path):\n",
    "    image_tensor = preprocess(image_path)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "    \n",
    "    output = output.squeeze().cpu().numpy()  # Remove batch dimension\n",
    "    if len(output.shape) == 3:  # Handle multi-class output\n",
    "        segmentation_mask = np.argmax(output, axis=0)  # Get the class with highest probability for each pixel\n",
    "    else:\n",
    "        segmentation_mask = (output > 0.5).astype(int)  # For binary output\n",
    "    \n",
    "    return segmentation_mask\n",
    "\n",
    "def get_color_map():\n",
    "    # Define a color map for each class\n",
    "    return np.array([\n",
    "        [0, 0, 0],        # black for 'cardboard'\n",
    "        [0, 255, 0],      # green for 'glass'\n",
    "        [255, 0, 0],      # red for 'metal'\n",
    "        [0, 0, 255],      # blue for 'paper'\n",
    "        [255, 255, 0],    # yellow for 'plastic'\n",
    "        [255, 0, 255]     # magenta for 'trash'\n",
    "    ], dtype=np.uint8)  # Ensure colors are in uint8 format\n",
    "\n",
    "def get_labels():\n",
    "    return ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
    "\n",
    "def overlay_labels(image_path, mask):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    color_map = get_color_map()\n",
    "    labels = get_labels()\n",
    "\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 15)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    for i in range(6):  # Assuming 6 classes\n",
    "        mask_class = mask == i\n",
    "        if np.any(mask_class):\n",
    "            coords = np.argwhere(mask_class)\n",
    "            print(f\"Coordinates for class {i}: {coords}\")  # Debug output\n",
    "            \n",
    "            if coords.shape[0] == 0:\n",
    "                print(f\"No coordinates found for class {i}\")\n",
    "                continue\n",
    "\n",
    "            # Ensure coords is in expected format\n",
    "            if coords.shape[1] != 2:\n",
    "                coords = coords.reshape(-1, 2)  # Ensure 2D shape if possible\n",
    "\n",
    "            min_x, min_y = coords.min(axis=0)\n",
    "            max_x, max_y = coords.max(axis=0)\n",
    "            \n",
    "            # Ensure valid coordinates\n",
    "            min_x, min_y, max_x, max_y = int(min_x), int(min_y), int(max_x), int(max_y)\n",
    "\n",
    "            # Draw bounding box and text\n",
    "            draw.rectangle([min_y, min_x, max_y, max_x], outline=tuple(color_map[i]), width=2)\n",
    "            draw.text((min_y, min_x), labels[i], fill=tuple(color_map[i]), font=font)\n",
    "\n",
    "    return image\n",
    "\n",
    "# Example usage\n",
    "image_path = '.\\\\test.jpg'\n",
    "segmentation_mask = segment_objects(image_path)\n",
    "annotated_image = overlay_labels(image_path, segmentation_mask)\n",
    "annotated_image.show()  # Show the annotated image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7ef123c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahan\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sahan\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object 0: Bounding box: (721, 309), (811, 658)\n",
      "Object 1: Bounding box: (837, 338), (907, 656)\n",
      "Object 2: Bounding box: (438, 328), (548, 684)\n",
      "Object 3: Bounding box: (766, 335), (860, 675)\n",
      "Object 4: Bounding box: (65, 451), (354, 644)\n",
      "Object 5: Bounding box: (842, 339), (902, 494)\n",
      "Object 6: Bounding box: (930, 550), (1136, 677)\n",
      "Object 7: Bounding box: (806, 335), (876, 666)\n",
      "Object 8: Bounding box: (0, 475), (1187, 755)\n",
      "Object 9: Bounding box: (56, 525), (341, 617)\n",
      "Object 10: Bounding box: (53, 592), (321, 649)\n",
      "Object 11: Bounding box: (61, 575), (345, 631)\n",
      "Object 12: Bounding box: (771, 555), (861, 678)\n",
      "Object 13: Bounding box: (56, 558), (354, 678)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "\n",
    "# Load your GarbageClassifier model\n",
    "model = torch.load('GarbageClassifier.pth')\n",
    "model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define classes for your GarbageClassifier\n",
    "classes = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
    "\n",
    "# Load pre-trained Faster R-CNN model for object detection\n",
    "detection_model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "detection_model.eval()\n",
    "detection_model.to(device)\n",
    "\n",
    "# Define preprocessing for GarbageClassifier\n",
    "def preprocess(image):\n",
    "    transform = T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.ToTensor(),\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "# Function to classify objects\n",
    "def predict_image(img, model):\n",
    "    xb = img.unsqueeze(0).to(device)  # Convert to batch of 1 and move to device\n",
    "    with torch.no_grad():\n",
    "        yb = model(xb)\n",
    "    prob, preds = torch.max(yb[0], dim=1)  # Get the predicted class\n",
    "    return classes[preds[0].item()]\n",
    "\n",
    "# Process image using Faster R-CNN and GarbageClassifier\n",
    "def process_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    color_map = np.array([\n",
    "        [0, 0, 0],        # black for 'cardboard'\n",
    "        [0, 255, 0],      # green for 'glass'\n",
    "        [255, 0, 0],      # red for 'metal'\n",
    "        [0, 0, 255],      # blue for 'paper'\n",
    "        [255, 255, 0],    # yellow for 'plastic'\n",
    "        [255, 0, 255]     # magenta for 'trash'\n",
    "    ], dtype=np.uint8)\n",
    "\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    # Preprocess image for object detection\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "    ])\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        detections = detection_model(image_tensor)[0]\n",
    "\n",
    "    # Draw bounding boxes and labels\n",
    "    for i, (box, label, score) in enumerate(zip(detections['boxes'], detections['labels'], detections['scores'])):\n",
    "        if score > 0.5:  # Confidence threshold\n",
    "            min_x, min_y, max_x, max_y = box.int().cpu().numpy()\n",
    "\n",
    "            # Debugging: Print the coordinates and class\n",
    "            print(f\"Object {i}: Bounding box: ({min_x}, {min_y}), ({max_x}, {max_y})\")\n",
    "\n",
    "            # Extract the object region\n",
    "            obj_region = image_tensor[:, min_y:max_y, min_x:max_x]\n",
    "            if obj_region.shape[1] == 0 or obj_region.shape[2] == 0:\n",
    "                continue  # Skip empty regions\n",
    "\n",
    "            # Classify the object\n",
    "            obj_label = predict_image(obj_region.squeeze(0), model)\n",
    "            print(f\"Object {i}: Classified as {obj_label}\")\n",
    "\n",
    "            # Draw bounding box and label\n",
    "            draw.rectangle([min_x, min_y, max_x, max_y], outline=tuple(color_map[classes.index(obj_label)]), width=2)\n",
    "            draw.text((min_x, min_y), obj_label, fill=tuple(color_map[classes.index(obj_label)]), font=font)\n",
    "\n",
    "    return image\n",
    "\n",
    "# Example usage\n",
    "image_path = '.\\\\test.jpg'\n",
    "annotated_image = process_image(image_path)\n",
    "annotated_image.show()  # Show the annotated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08079095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sasan\\AppData\\Local\\Temp\\ipykernel_1416\\1440616192.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('GarbageClassifier.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects: 45\n",
      "Object 0: Bounding box: tensor([721.3423, 309.4503, 812.0912, 658.3715]), Score: 0.9944170713424683\n",
      "Object 1: Bounding box: tensor([837.5850, 338.4106, 907.4632, 657.1381]), Score: 0.9919330477714539\n",
      "Object 2: Bounding box: tensor([439.0090, 328.3222, 549.0280, 683.6404]), Score: 0.977828860282898\n",
      "Object 3: Bounding box: tensor([766.8976, 337.0799, 860.4631, 675.6911]), Score: 0.961835503578186\n",
      "Object 4: Bounding box: tensor([ 64.9997, 451.3032, 354.7159, 643.7771]), Score: 0.9197129607200623\n",
      "Object 5: Bounding box: tensor([842.0444, 339.2059, 902.1689, 494.5328]), Score: 0.9180073738098145\n",
      "Object 6: Bounding box: tensor([ 930.7585,  548.6348, 1136.9679,  677.1661]), Score: 0.8908308744430542\n",
      "Object 7: Bounding box: tensor([806.6801, 335.7841, 876.8629, 666.4393]), Score: 0.8302330374717712\n",
      "Object 8: Bounding box: tensor([   0.0000,  475.5961, 1187.0000,  755.3837]), Score: 0.7574615478515625\n",
      "Object 9: Bounding box: tensor([ 56.7125, 525.4404, 341.2184, 617.7027]), Score: 0.6521540880203247\n",
      "Object 10: Bounding box: tensor([ 53.2899, 591.7515, 323.8679, 651.3610]), Score: 0.6343586444854736\n",
      "Object 11: Bounding box: tensor([771.1795, 556.0899, 861.4122, 678.5583]), Score: 0.6018356680870056\n",
      "Object 12: Bounding box: tensor([ 61.8180, 575.3846, 345.1274, 631.7604]), Score: 0.601249635219574\n",
      "Object 13: Bounding box: tensor([ 56.9022, 558.4006, 354.5809, 678.8892]), Score: 0.5456209778785706\n",
      "Object 14: Bounding box: tensor([603.0496, 505.6650, 706.6281, 662.4071]), Score: 0.45380669832229614\n",
      "Object 15: Bounding box: tensor([ 925.2892,  441.0134, 1153.4531,  631.6669]), Score: 0.4306621849536896\n",
      "Object 16: Bounding box: tensor([ 64.2232, 592.7491, 286.7673, 627.3453]), Score: 0.40089860558509827\n",
      "Object 17: Bounding box: tensor([587.5477, 426.7991, 671.9947, 519.8320]), Score: 0.39932313561439514\n",
      "Object 18: Bounding box: tensor([376.2409, 558.5298, 540.6283, 681.5814]), Score: 0.3493914306163788\n",
      "Object 19: Bounding box: tensor([367.4950, 580.0046, 429.0876, 642.0156]), Score: 0.33269065618515015\n",
      "Object 20: Bounding box: tensor([ 225.0721,  223.4562, 1185.7213,  775.7329]), Score: 0.28114262223243713\n",
      "Object 21: Bounding box: tensor([372.6181, 547.9832, 546.3412, 680.3394]), Score: 0.26077133417129517\n",
      "Object 22: Bounding box: tensor([365.8888, 583.5187, 431.8119, 643.6121]), Score: 0.221834197640419\n",
      "Object 23: Bounding box: tensor([ 928.7391,  449.9987, 1145.8500,  667.2585]), Score: 0.2137637585401535\n",
      "Object 24: Bounding box: tensor([ 88.0030, 652.6198, 319.1146, 684.1154]), Score: 0.20772457122802734\n",
      "Object 25: Bounding box: tensor([ 965.3398,  491.9022, 1008.7709,  530.8095]), Score: 0.20610107481479645\n",
      "Object 26: Bounding box: tensor([859.7021, 548.9821, 902.5892, 657.7807]), Score: 0.202586829662323\n",
      "Object 27: Bounding box: tensor([ 51.0016, 631.9736, 328.6101, 676.9243]), Score: 0.18747027218341827\n",
      "Object 28: Bounding box: tensor([556.7895, 539.6857, 656.2094, 691.7179]), Score: 0.1813974529504776\n",
      "Object 29: Bounding box: tensor([  3.5048, 378.7570, 684.1477, 755.8710]), Score: 0.1636641025543213\n",
      "Object 30: Bounding box: tensor([796.6388, 323.6402, 843.7454, 498.3707]), Score: 0.14628472924232483\n",
      "Object 31: Bounding box: tensor([ 219.8100,  584.4963, 1180.2949,  759.6064]), Score: 0.12902188301086426\n",
      "Object 32: Bounding box: tensor([ 927.4880,  446.6630, 1115.2621,  496.9533]), Score: 0.12263858318328857\n",
      "Object 33: Bounding box: tensor([ 933.2725,  439.0683, 1129.9142,  552.7771]), Score: 0.12182825058698654\n",
      "Object 34: Bounding box: tensor([737.3575, 318.5610, 915.3529, 674.8090]), Score: 0.10429815948009491\n",
      "Object 35: Bounding box: tensor([757.4262, 298.2663, 796.5564, 434.3524]), Score: 0.0891033187508583\n",
      "Object 36: Bounding box: tensor([ 942.9377,  462.7369, 1143.9874,  603.0914]), Score: 0.07321678102016449\n",
      "Object 37: Bounding box: tensor([ 58.8430, 627.0533, 304.1866, 659.2194]), Score: 0.07013126462697983\n",
      "Object 38: Bounding box: tensor([376.7343, 563.3813, 546.8046, 683.8051]), Score: 0.06965142488479614\n",
      "Object 39: Bounding box: tensor([ 66.1283, 649.0449, 327.4446, 687.7984]), Score: 0.06515665352344513\n",
      "Object 40: Bounding box: tensor([858.6315, 560.6264, 902.9728, 657.1262]), Score: 0.06291745603084564\n",
      "Object 41: Bounding box: tensor([584.2639, 424.8390, 678.0756, 533.1078]), Score: 0.06279133260250092\n",
      "Object 42: Bounding box: tensor([651.2177, 619.8378, 705.7811, 631.4406]), Score: 0.060036566108465195\n",
      "Object 43: Bounding box: tensor([373.2005, 568.5016, 466.5459, 650.7392]), Score: 0.056788183748722076\n",
      "Object 44: Bounding box: tensor([651.5035, 623.7996, 704.8994, 637.4756]), Score: 0.050665732473134995\n",
      "Drawing bounding box: (721, 309), (812, 658)\n",
      "Drawing bounding box: (837, 338), (907, 657)\n",
      "Drawing bounding box: (439, 328), (549, 683)\n",
      "Drawing bounding box: (766, 337), (860, 675)\n",
      "Drawing bounding box: (64, 451), (354, 643)\n",
      "Drawing bounding box: (842, 339), (902, 494)\n",
      "Drawing bounding box: (930, 548), (1136, 677)\n",
      "Drawing bounding box: (806, 335), (876, 666)\n",
      "Drawing bounding box: (0, 475), (1187, 755)\n",
      "Drawing bounding box: (56, 525), (341, 617)\n",
      "Drawing bounding box: (53, 591), (323, 651)\n",
      "Drawing bounding box: (771, 556), (861, 678)\n",
      "Drawing bounding box: (61, 575), (345, 631)\n",
      "Drawing bounding box: (56, 558), (354, 678)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "\n",
    "# Load your GarbageClassifier model\n",
    "model = torch.load('GarbageClassifier.pth')\n",
    "model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define classes for your GarbageClassifier\n",
    "classes = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
    "\n",
    "# Load pre-trained Faster R-CNN model for object detection\n",
    "detection_model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "detection_model.eval()\n",
    "detection_model.to(device)\n",
    "\n",
    "# Define preprocessing for GarbageClassifier\n",
    "def preprocess(image):\n",
    "    transform = T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.ToTensor(),\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "# Function to classify objects\n",
    "def predict_image(img, model):\n",
    "    xb = img.unsqueeze(0).to(device)  # Convert to batch of 1 and move to device\n",
    "    with torch.no_grad():\n",
    "        yb = model(xb)\n",
    "    prob, preds = torch.max(yb[0], dim=1)  # Get the predicted class\n",
    "    return classes[preds[0].item()]\n",
    "\n",
    "# Process image using Faster R-CNN and GarbageClassifier\n",
    "def process_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    color_map = np.array([\n",
    "        [0, 0, 0],        # black for 'cardboard'\n",
    "        [0, 255, 0],      # green for 'glass'\n",
    "        [255, 0, 0],      # red for 'metal'\n",
    "        [0, 0, 255],      # blue for 'paper'\n",
    "        [255, 255, 0],    # yellow for 'plastic'\n",
    "        [255, 0, 255]     # magenta for 'trash'\n",
    "    ], dtype=np.uint8)\n",
    "\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    # Preprocess image for object detection\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "    ])\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        detections = detection_model(image_tensor)[0]\n",
    "\n",
    "    # Debugging: Print total detected objects and their scores\n",
    "    print(f\"Detected objects: {len(detections['boxes'])}\")\n",
    "    for i, (box, label, score) in enumerate(zip(detections['boxes'], detections['labels'], detections['scores'])):\n",
    "        print(f\"Object {i}: Bounding box: {box}, Score: {score}\")\n",
    "\n",
    "    # Draw bounding boxes and labels\n",
    "    for i, (box, label, score) in enumerate(zip(detections['boxes'], detections['labels'], detections['scores'])):\n",
    "        if score > 0.5:  # Confidence threshold\n",
    "            min_x, min_y, max_x, max_y = box.int().cpu().numpy()\n",
    "\n",
    "            # Debugging: Print bounding box details\n",
    "            print(f\"Drawing bounding box: ({min_x}, {min_y}), ({max_x}, {max_y})\")\n",
    "\n",
    "            # Extract the object region\n",
    "            obj_region = image_tensor[:, min_y:max_y, min_x:max_x]\n",
    "            if obj_region.shape[1] == 0 or obj_region.shape[2] == 0:\n",
    "                continue  # Skip empty regions\n",
    "\n",
    "            # Resize object region to match classifier input size\n",
    "            obj_region = T.Resize((256, 256))(obj_region.squeeze(0))  # Resize to classifier input size\n",
    "\n",
    "            # Classify the object\n",
    "            obj_label = predict_image(obj_region.unsqueeze(0), model)\n",
    "            print(f\"Object {i}: Classified as {obj_label}\")\n",
    "\n",
    "            # Draw bounding box and label\n",
    "            draw.rectangle([min_x, min_y, max_x, max_y], outline=tuple(color_map[classes.index(obj_label)]), width=2)\n",
    "            draw.text((min_x, min_y - 10), obj_label, fill=tuple(color_map[classes.index(obj_label)]), font=font)\n",
    "\n",
    "    return image\n",
    "\n",
    "# Example usage\n",
    "image_path = '.\\\\test.jpg'\n",
    "annotated_image = process_image(image_path)\n",
    "annotated_image.show()  # Show the annotated image\n",
    "annotated_image.save('annotated_image_with_labels.jpg')  # Save the annotated image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9735dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from PIL import Image, ImageDraw\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Load pre-trained Faster R-CNN model for object detection\n",
    "detection_model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "detection_model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "detection_model.to(device)\n",
    "\n",
    "# Function to process image and draw bounding boxes\n",
    "def process_image(image_path):\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "    ])\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Perform object detection\n",
    "    with torch.no_grad():\n",
    "        detections = detection_model(image_tensor)[0]\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    for i, (box, score) in enumerate(zip(detections['boxes'], detections['scores'])):\n",
    "        if score > 0.5:  # Confidence threshold\n",
    "            min_x, min_y, max_x, max_y = box.int().cpu().numpy()\n",
    "            \n",
    "            # Draw bounding box\n",
    "            draw.rectangle([min_x, min_y, max_x, max_y], outline=\"black\", width=2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Example usage\n",
    "image_path = '.\\\\test.jpg'\n",
    "annotated_image = process_image(image_path)\n",
    "annotated_image.show()  # Show the annotated image\n",
    "annotated_image.save('gimage.jpg')  # Save the annotated image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e12cbeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sasan\\AppData\\Local\\Temp\\ipykernel_1416\\1070472941.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  garbage_model = torch.load('GarbageClassifier.pth')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import cv2\n",
    "from io import BytesIO\n",
    "\n",
    "# Load your GarbageClassifier model\n",
    "garbage_model = torch.load('GarbageClassifier.pth')\n",
    "garbage_model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "garbage_model.to(device)\n",
    "\n",
    "# Define classes for your GarbageClassifier\n",
    "classes = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
    "\n",
    "# Load pre-trained Faster R-CNN model for object detection\n",
    "detection_model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "detection_model.eval()\n",
    "detection_model.to(device)\n",
    "\n",
    "# Define preprocessing for GarbageClassifier\n",
    "def preprocess_image(image):\n",
    "    transform = T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.ToTensor(),\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "# Function to classify objects\n",
    "def predict_image(img, model):\n",
    "    xb = img.unsqueeze(0).to(device)  # Convert to batch of 1 and move to device\n",
    "    with torch.no_grad():\n",
    "        yb = model(xb)\n",
    "    prob, preds = torch.max(yb[0], dim=1)  # Get the predicted class\n",
    "    return classes[preds[0].item()]\n",
    "\n",
    "# Function to extract and classify object regions\n",
    "def process_image(image_path, model, classes, device):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    width, height = image.size\n",
    "\n",
    "    # Define transforms\n",
    "    preprocess = T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # Dummy function for object detection\n",
    "    def dummy_detect_objects(image):\n",
    "        # Replace this with your actual object detection code\n",
    "        return [(50, 50, 150, 150), (200, 200, 300, 300)]  # Example bounding boxes\n",
    "\n",
    "    # Detect objects in the image\n",
    "    detections = dummy_detect_objects(image)\n",
    "\n",
    "    # Prepare to draw bounding boxes and labels\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Classification transform\n",
    "    classify_transform = T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    for i, (min_x, min_y, max_x, max_y) in enumerate(detections):\n",
    "        # Crop the object region from the image\n",
    "        obj_region = image.crop((min_x, min_y, max_x, max_y))\n",
    "        obj_region = classify_transform(obj_region).unsqueeze(0).to(device)\n",
    "\n",
    "        # Classify the object\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(obj_region)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            label = classes[preds.item()]\n",
    "\n",
    "        # Draw bounding box and label\n",
    "        draw.rectangle([min_x, min_y, max_x, max_y], outline=\"black\", width=2)\n",
    "        draw.text((min_x, min_y), label, fill=\"black\")\n",
    "\n",
    "    return image\n",
    "\n",
    "# Example usage\n",
    "image_path = '.\\\\test.jpg'\n",
    "annotated_image = process_image(image_path, garbage_model, classes, device)\n",
    "annotated_image.show()  # Show the annotated image\n",
    "annotated_image.save('annotated_image_with_labels.jpg')  # Save the annotated image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a899cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def to_device(tensor, device):\n",
    "    return tensor.to(device)\n",
    "\n",
    "# Initialize your model and device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "garbage_model = torch.load(\"GarbageClassifier.pth\").to(device)  # Load your trained model\n",
    "garbage_model.eval()\n",
    "\n",
    "# Load a pre-trained Faster R-CNN model for object detection\n",
    "detection_model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "detection_model.to(device)\n",
    "detection_model.eval()\n",
    "\n",
    "def detect_objects(image, model, device):\n",
    "    image_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image_tensor)\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    threshold = 0.5\n",
    "    boxes = boxes[scores > threshold]\n",
    "    return [tuple(box) for box in boxes]\n",
    "\n",
    "def preprocess_image(image):\n",
    "    transform = T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "def predict_image(img, model):\n",
    "    xb = to_device(img.unsqueeze(0), device)\n",
    "    with torch.no_grad():\n",
    "        yb = model(xb)\n",
    "        prob, preds = torch.max(yb, dim=1)\n",
    "    return classes[preds[0].item()]\n",
    "\n",
    "def process_image(image_path, model, classes, device):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    detections = detect_objects(image, detection_model, device)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    for min_x, min_y, max_x, max_y in detections:\n",
    "        obj_region = image.crop((min_x, min_y, max_x, max_y))\n",
    "        obj_region_preprocessed = preprocess_image(obj_region)\n",
    "        label = predict_image(obj_region_preprocessed, model)\n",
    "\n",
    "        draw.rectangle([min_x, min_y, max_x, max_y], outline=\"black\", width=2)\n",
    "        draw.text((min_x, min_y - 20), label, fill=\"black\", font=font)\n",
    "\n",
    "    return image\n",
    "\n",
    "# Example usage\n",
    "classes = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']  # Define your classes\n",
    "image_path = '.\\\\cimage.jpg'\n",
    "annotated_image = process_image(image_path, garbage_model, classes, device)\n",
    "annotated_image.show()  # Show the annotated image\n",
    "annotated_image.save('annotated_image_with_labels.jpg')  # Save the annotated image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adea7d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
